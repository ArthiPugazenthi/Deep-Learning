# -*- coding: utf-8 -*-
"""Flax_DeepLearning_Framework.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10uEuEKKUkoqNWq-60CTl7k3TuOq_l3QN

Import libraries
"""

!pip install datasets  # Install the 'datasets' library

# Core libraries
import jax  # Main JAX library for numerical computing
import jax.numpy as jnp  # JAX NumPy for array operations
import flax.linen as nn  # Flax module for defining neural networks
import optax  # Optimizer library for JAX, Used to update model parameters during training.
import numpy as np  # Standard NumPy for general array operations
import matplotlib.pyplot as plt  # Matplotlib for visualization
from flax.training import train_state  # Module for managing model training state
from datasets import load_dataset  # Load datasets from Hugging Face

"""Load MNIST dataset from Hugging Face and preprocess it"""

def get_data():
    ds = load_dataset("mnist")  # Load MNIST dataset (images & labels)

    # Convert images to NumPy arrays, normalize (0-255 -> 0-1), and stack them into a JAX array
    X_train = jnp.stack([jnp.array(img, dtype=jnp.float32) / 255.0 for img in ds['train']['image']])
    y_train = jnp.array(ds['train']['label'], dtype=jnp.int32)  # Convert labels to JAX array (int32)

    X_test = jnp.stack([jnp.array(img, dtype=jnp.float32) / 255.0 for img in ds['test']['image']])
    y_test = jnp.array(ds['test']['label'], dtype=jnp.int32)

    return (X_train, y_train), (X_test, y_test)  # Return training & test data

"""Loads the MNIST dataset.
split='train' → Loads the training dataset.
as_supervised=True → Ensures dataset returns tuples (image, label).

Define a simple neural network using Flax
"""

class Net(nn.Module):
    @nn.compact  # Decorator to simplify layer definitions
    def __call__(self, x):
        x = x.reshape((x.shape[0], -1))  # Flatten 2D image into a 1D vector
        x = nn.Dense(128)(x)  # Fully connected layer with 128 neurons
        x = nn.relu(x)  # Apply ReLU activation function, Formula: ReLU(x) = max(0, x)
        return nn.Dense(10)(x)  # Output layer with 10 neurons (0 - 9)

"""Create the model and initialize parameters"""

def create_state():
    model = Net()  # Instantiate the neural network
    params = model.init(jax.random.PRNGKey(0), jnp.ones((1, 28, 28)))['params']  # Initialize parameters
    return train_state.TrainState.create( # Adjust indentation here to align with 'def'
        apply_fn=model.apply,  # Function to apply the model
        params=params,         # The initialized parameters
        tx=optax.adam(0.001)   # Adam optimizer with learning rate = 0.001
    )

"""Define a single training step using JAX"""

@jax.jit  # Optimize the function with Just-In-Time (JIT) compilation
def train(state, x, y):
    def loss_fn(p):
        logits = state.apply_fn({'params': p}, x)  # Forward pass: Compute predictions
        return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, y))  # Compute loss

    grads = jax.grad(loss_fn)(state.params)  # Compute gradients using automatic differentiation
    return state.apply_gradients(grads=grads)  # Update model parameters

"""Softmax turns logits into class probabilities.

Cross-entropy compares predictions with actual labels.

Lower loss = better model performance.

Load dataset and initialize model
"""

(X_train, y_train), (X_test, y_test) = get_data()
state = create_state()
batch_size = 128  # Define batch size
for i in range(0, len(X_train), batch_size):
    xb, yb = X_train[i:i+batch_size], y_train[i:i+batch_size]  # Get batch data
    state = train(state, xb, yb)  # Perform a training step

print("✅ Training done")  # Indicate training completion

"""Visualize model predictions on test images"""

num_images = 8  # Number of test images to display
fig, axes = plt.subplots(1, num_images, figsize=(12, 3))  # Create a horizontal subplot

for i in range(num_images):
    img = X_test[i].reshape(28, 28)  # Reshape to 2D image format
    pred = jnp.argmax(state.apply_fn({'params': state.params}, jnp.expand_dims(X_test[i], 0)))  # Predict the label

    axes[i].imshow(img, cmap='gray')  # Show image in grayscale
    axes[i].set_title(f"Pred: {pred}")  # Display predicted label
    axes[i].axis("off")  # Hide axes

plt.tight_layout()  # Adjust layout to avoid overlap
plt.show()  # Display the images

""" Simple Flax Neural Network"""

import jax.numpy as jnp   #JAX-compatible version of NumPy
import flax.linen as nn   #high-level neural network module in Flax

# Define a simple neural network  class inherits from nn.Module, which is the base class for defining neural networks in Flax.
class SimpleNN(nn.Module):  #nn.Module provides structure and useful functionalities like parameter handling, initialization, and execution.
    @nn.compact
    def __call__(self, x):
        x = nn.Dense(16)(x)  # Hidden layer with 16 neurons
        x = nn.relu(x)
        x = nn.Dense(1)(x)  # Output layer (1 neuron)
        return nn.sigmoid(x)

# Create a model instance
model = SimpleNN()

# Dummy input data
x = jnp.ones((1, 10))  # 1 sample, 10 features

# Initialize model parameters
params = model.init(jax.random.PRNGKey(0), x)

# Forward pass (prediction)
output = model.apply(params, x)
print("Output:", output)
